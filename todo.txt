X Implementing additional scenarios:
	expanded scenarios:
		local
			PIFSC ethernet
				pifsc-ethernet
			VPN
				pacific-vpn
		hybrid
			PIFSC ethernet
				pifsc-ethernet
			VPN
				pacific-vpn
				east-vpn
				west-vpn
		remote
			FishSTOC network
		

X Update documentation to reference the shell scripts for the differet scenarios and the different config files (.sql, etc.)



* X Simply rename the config files based on the testing_scenario and path_network_connection and then remove the other files using wildcards/regexp:
	
X update the traceroute to capture separate logs for each network/location scenario






















X develop automation schedule (cron job) for executing the existing docker images (hourly between 7 AM to 7 PM each day)
	X Implement as a Windows scheduled task


X Implement TNSNames.ora for the docker container	


X Specify V_DB_NAME in a configuration.sql file so the user doesn't have to change the calling script
	They could also change the output file name so there doesn't need to separate calling scripts at all for the forked repositories

X Change deployment scripts so they use variables instead (e.g. repository URL, deployment path, etc.)
	X This way the configuration files change but the main scripts will be the same as the upstream repository)
	
	
Elapsed time of the query
	grant select any dictionary to LHP_INTL_BIO;
	select SQL_TEXT, elapsed_time from v$sql where SQL_TEXT LIKE '%%';
		https://docs.oracle.com/en/database/oracle/oracle-database/19/refrn/V-SQL.html
		Elapsed database time (in microseconds) used by this cursor for parsing, executing, and fetching. If the cursor uses parallel execution, then ELAPSED_TIME is the cumulative time for the query coordinator, plus all parallel query slave processes.
		- *Note: after I converted this number was larger than the timer (e.g. 53.8s for elapsed_time and 1.368 to download the .csv data)

		AUTOTRACE didn't provide execution time
		
		SET TIMING ON didn't provide execution time

	Tried using DBA_HIST_SQLSTAT to get CPU_TIME_TOTAL, ELAPSED_TIME_TOTAL from the query that was used to export the data in .csv format but sometimes it would return no rows or many rows for the same SQL_ID value
		This was not a reliable way to get the execution time
		Also, the _TOTAL columns are cumulative for each time the query was executed
		_DELTA columns are just the difference between the last time the query was executed and this time the query was executed, it does not provide the total time the 
		
	Tried to use SQL Trace but it creates files on the server that need to be converted to human-readable values using a command line utility, this is not feasible for automated extraction of timing information

X implement traceroute loop while the queries are executing





Ahi deployment:

	cd /tmp/jabdul
	# remove the working directory (if any)
	rm -rf *
	
	# clone the repository
	git clone --branch branch_OCI_deployment https://github.com/noaa-pifsc/PIFSC-Tools-SQLPlus-Query-Metrics-IBBS.git
	
	# run the deployment script
	bash ./PIFSC-Tools-SQLPlus-Query-Metrics-IBBS/deployment_scripts/prepare_docker_project.local.sh
	
	# when prompted enter the following:
	/home/webd/docker/pirrid
	
	# update the credentials file:
	vim /home/webd/docker/pirrid/docker/sqlplus-query-metrics-ibbs-local/docker/src/SQL/credentials/DB_credentials.sql
	
	# specify the credentials:
	
	
	# run the docker container:
	bash /home/webd/docker/pirrid/docker/sqlplus-query-metrics-ibbs-local/deployment_scripts/build_deploy_project.sh
	
	
	
	
# show all docker containers
docker container ls -a.



docker volume copy (source: https://stackoverflow.com/questions/35406213/how-to-copy-data-from-docker-volume-to-host)
	update Dockerfile to leave it running once it's executed (https://stackoverflow.com/questions/25775266/how-to-keep-docker-container-running-after-starting-services):
	
	# update Dockerfile
		vim /home/webd/docker/pirrid/sqlplus-query-metrics-ibbs-local/docker/Dockerfile	
	
	# add the following at the end fo Dockerfile:
		
		ENTRYPOINT ["tail", "-f", "/dev/null"]
		
	# run the container
		bash /home/webd/docker/pirrid/sqlplus-query-metrics-ibbs-local/deployment_scripts/build_deploy_project.sh
		
	# connect to the container:
		docker exec -it [CONTAINER_ID] bash
		
		
	# wait until container is finished executing, then change the permissions on the container file:
		chmod 777 ../data_exports/ibbs-query-metrics.csv
		
	# exit from the bash session
		exit
	

	# copy the file 
		docker cp [CONTAINER_ID]:/usr/src/data_exports/ibbs-query-metrics.csv /tmp/jabdul/ibbs-query-metrics.csv
	
	# shutdown the container
		docker container stop [CONTAINER_ID]
	
	# update the Dockerfile to revert the changes (comment out ENTRYPOINT line):
		
		vim /home/webd/docker/pirrid/sqlplus-query-metrics-ibbs-local/docker/Dockerfile	
		
	
	




	
docker volume copy (source: https://stackoverflow.com/questions/35406213/how-to-copy-data-from-docker-volume-to-host)
	cmd /c ""C:\Program Files\Git\bin\bash.exe"

	CID=$(docker run -d -v docker_sqlplus-query-metrics-ibbs-data:/docker_sqlplus-query-metrics-ibbs-data busybox true)
	

	sudo docker cp $CID:/docker_sqlplus-query-metrics-ibbs-data ./

	docker rm $CID
	
	
	docker rm ./
	
	
	
testing (9/18/24):
	Linux (ahi)
		X hybrid
		X remote
		X local
	
	Windows (PICV014)
		X hybrid
		X remote
		X local
	
	
	
	
	# what is the best way to configure the preparation script (simple variable declaration and then call the other script)
	
	



List of things to do:
	Consolidate more of the logic in the docker/src/SQL/query_metrics_export.sql file from the calling scripts where feasible (move all of the non-scenario-specific code - login, setting options, loading configuration, etc.)
		May be able to just call the consolidated script if it just reads variable values defined in the calling script (make it like the project_scenario_config.$scenario.sh scripts)
	look for other consolidation opportunities
	
	implement the upstream repository and implement the improvements from this IBBS fork of the SQLPlus query metrics project
	